{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Artificial Neural Network (ANN) - Part I\n",
    "\n",
    "## 1. ANN Definition\n",
    "\n",
    "Artificial Neural Networks are models that try to mimic some of the human brain behaviors, specifically how to acquire and store that knowledge. From a process called training, the ANN can extract relationships and patterns found on a set of historical samples, and generalize this to samples not seen before. For example, imagine one can train an ANN to model the patterns that characterize a chronic disease from a set of past medical exams. Once the model is trained, it can predict the chance of a new exam be from an ill patient. Besides classification, ANN has been successfully applied in several other domains like Regression, clustering, time-series forecasting, and so on. Nowadays, ANN has evolved into Deep Learning, the cutting-edge technology used to control self-driving cars and to perform image classification."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Perceptron\n",
    "\n",
    "The Perceptron is the fundamental rock to anyone who wants to goes deeper into Artificial Neural Network and Deep Learning. The Perceptron network is classified as a Feedforward single-layer network, is based on the McCulloch & Pitts (1943). Figure 1 depicts the MPC neuron:\n",
    "\n",
    "<img src=\"images/perceptron01.png\" width=\"55%\" title=\"MCP Neuron\">\n",
    "\n",
    "The MCP neuron has the main parts depicted next:\n",
    "\n",
    "**a) Input**: $[x_1, x_2,..., x_n]$\n",
    "\n",
    "Signs or values from the environment (application). For example, in an application to predict houses prices, $x_1$ could be the size of the house at $m^2$, while $x_2$ could be the number of bedrooms.\n",
    "\n",
    "**b) Weights**: $[w_1, w_2,..., w_n]$\n",
    "\n",
    "Values used to weight the importance of each input variable over the neuron output. For example, considering the house prices prediction application, $w_1$, and $w_2$ would weight how significantly the size and number of bedrooms (respectively) are to predict the price.\n",
    "\n",
    "**c) Linear combination**: $\\sum$\n",
    "\n",
    "To sum the weighted inputs to generate a activation potential.\n",
    "\n",
    "**d) Bias(Activation threshold)**: $-\\theta$\n",
    "\n",
    "A threshold to limit the value of the linear combination.\n",
    "\n",
    "**e) Action Potential**: $u$\n",
    "\n",
    "The result from the diference between the linear combination and the bias.\n",
    "\n",
    "**f) Activation function**: $g(u)$\n",
    "\n",
    "This function is responsible to generate the output in a appropriate interval range for the application.\n",
    "\n",
    "**g) Output**: $y$\n",
    "\n",
    "The final value generated by the neuron."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Perceptron code\n",
    "\n",
    "First, we have to compute the action potential, which is given by $u = \\sum_{i=1}^n x_i \\cdot w_i - \\theta$. \n",
    "\n",
    "Once, the values of inputs and weights are in the form of a matrix, we could simple perform a matrix multiplication, given by: \n",
    "\n",
    "$\\sum_{i=1}^n = [w_1, w_2, ..., w_n]^T \\cdot [x_1, x_2, ..., x_n]$\n",
    "\n",
    "After that, it would be necessary subtract $\\theta$ from $\\Sigma$:\n",
    "\n",
    "$u = \\sum - \\theta$\n",
    "\n",
    "In order to simply, we could simply add $-1$ to the input matrix: $[-1, w_1, w_2,..., w_n]$ and $\\theta$ to weights matrix: $[\\theta, w_1, w_2,..., w_n]$. At the end, we have:\n",
    "\n",
    "$u = \\sum_{i=0}^n x_i \\cdot w_i$\n",
    "\n",
    "### 2.1. Numpy library\n",
    "\n",
    "In python, the Numpy library gives us a powerfull set of resources to operate linear algebra operations. For example, consider we have the input given by $x = [0.1, 0.9, 0.5]$, the weights given by $[0.4, 0.3, 0.2]$ and $\\theta = -1.5$. The basic operation of our neuron could be simple: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-1.09\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "x = np.array([-1, 0.1, 0.9, 0.5])\n",
    "weights = np.array([1.5,0.4,0.3,0.2])\n",
    "\n",
    "u = np.matmul(x, weights)\n",
    "\n",
    "print(round(u,2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2. Version 1 of our Perceptron class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "class Perceptron:\n",
    "    \n",
    "    def __init__(self, input_size):\n",
    "        self.weights = np.random.normal(size=input_size+1)\n",
    "        self.inputs = []\n",
    "    \n",
    "    def bi_step(self, u):\n",
    "        return 1 if u >= 0 else -1\n",
    "    \n",
    "    def output(self, inputs):\n",
    "        inputs = np.append(-1, inputs)\n",
    "        u = np.matmul(self.weights, inputs)\n",
    "        return self.bi_step(u)\n",
    "\n",
    "\n",
    "\n",
    "sample = np.array([0.1, 0.9, 0.5])\n",
    "perceptron = Perceptron(3)\n",
    "y = perceptron.output(sample)\n",
    "print(y)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3. Training our model\n",
    "\n",
    "As said before, it is necessary an algorithm to train a model, based on a set of samples. In this tutorial, we will coven an basic algorithm called Hebb Rule."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.4. The complete code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.42958134  1.11889307 -1.24308107]\n",
      "-1\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "class Perceptron:\n",
    "    \n",
    "    def __init__(self, input_size, learning_rate = 0.05):\n",
    "        self.input_size = input_size\n",
    "        self.weights = np.random.normal(size=self.input_size+1)\n",
    "        self.inputs = []\n",
    "        self.epoch = 0\n",
    "        self.learning_rate = learning_rate\n",
    "        self.samples = np.array([])\n",
    "        self.outputs = np.array([])\n",
    "    \n",
    "    def bi_step(self, u):\n",
    "        return 1 if u >= 0 else -1\n",
    "    \n",
    "    def train(self):\n",
    "        if len(self.samples) == 0 or len(self.outputs) == 0:\n",
    "            raise Exception('Please, you must provide a dataset for training')\n",
    "\n",
    "        self.weights = np.random.normal(size=self.input_size+1)\n",
    "        error = True\n",
    "        self.epoch = 0\n",
    "\n",
    "        while error:\n",
    "            error = False\n",
    "    \n",
    "            for i, sample in enumerate(self.samples):\n",
    "                sample = np.append(-1, sample)\n",
    "                u = np.matmul(sample, self.weights)\n",
    "                y = self.bi_step(u)\n",
    "        \n",
    "                if y != self.outputs[i]:\n",
    "                    error = True\n",
    "                    self.weights = self.weights + self.learning_rate * (self.outputs[i] - y) * sample\n",
    "    \n",
    "            self.epoch += 1\n",
    "    \n",
    "    def output(self, inputs):\n",
    "        inputs = np.append(-1, inputs)\n",
    "        u = np.matmul(self.weights, inputs)\n",
    "        return self.bi_step(u)\n",
    "\n",
    "    \n",
    "    \n",
    "samples = np.array([[0.1, 0.4], \n",
    "                    [0.3, 0.7],\n",
    "                    [0.6, 0.9],\n",
    "                    [0.5, 0.7]])   \n",
    "\n",
    "outputs = np.array([1, -1, -1, 1])\n",
    "\n",
    "perceptron = Perceptron(len(samples[0]))\n",
    "\n",
    "perceptron.samples = samples\n",
    "perceptron.outputs = outputs\n",
    "\n",
    "perceptron.train()\n",
    "print(perceptron.weights)\n",
    "\n",
    "y = perceptron.output(np.array([0.3, 0.7]))\n",
    "print(y)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
